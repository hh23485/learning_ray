{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "780507ca",
      "metadata": {
        "id": "780507ca"
      },
      "source": [
        "# Reinforcement Learning with Ray RLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2856c7b0",
      "metadata": {
        "id": "2856c7b0"
      },
      "source": [
        "\n",
        "You can run this notebook directly in\n",
        "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_04_rllib.ipynb).\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_04_rllib.ipynb\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea5e1f7",
      "metadata": {
        "id": "3ea5e1f7"
      },
      "source": [
        "For this chapter you need to install the following dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a9d6d409",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9d6d409",
        "outputId": "0089d9ad-98d9-4c75-d20f-bf972b77e2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray==2.2.0 (from ray[rllib]==2.2.0)\n",
            "  Downloading ray-2.2.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (24.2.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (3.16.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (4.25.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (1.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (2.32.3)\n",
            "Collecting virtualenv>=20.0.24 (from ray==2.2.0->ray[rllib]==2.2.0)\n",
            "  Downloading virtualenv-20.28.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (1.68.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (24.2)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray==2.2.0->ray[rllib]==2.2.0) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.2.0) (2.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.2.0) (0.9.0)\n",
            "Collecting tensorboardX>=1.9 (from ray[rllib]==2.2.0)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.2.0) (0.1.8)\n",
            "Collecting gym<0.24.0,>=0.21.0 (from ray[rllib]==2.2.0)\n",
            "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lz4 (from ray[rllib]==2.2.0)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: matplotlib!=3.4.3 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.2.0) (3.8.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.2.0) (0.24.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.2.0) (1.13.1)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.2.0) (0.13.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from ray[rllib]==2.2.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<0.24.0,>=0.21.0->ray[rllib]==2.2.0) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<0.24.0,>=0.21.0->ray[rllib]==2.2.0) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.2.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.2.0) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.2.0) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.2.0) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.2.0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.4.3->ray[rllib]==2.2.0) (2.8.2)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.0.24->ray==2.2.0->ray[rllib]==2.2.0)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.0.24->ray==2.2.0->ray[rllib]==2.2.0) (4.3.6)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.2.0->ray[rllib]==2.2.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.2.0->ray[rllib]==2.2.0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.2.0->ray[rllib]==2.2.0) (0.21.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]==2.2.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]==2.2.0) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.2.0->ray[rllib]==2.2.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.2.0->ray[rllib]==2.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.2.0->ray[rllib]==2.2.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.2.0->ray[rllib]==2.2.0) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]==2.2.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]==2.2.0) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]==2.2.0) (4.12.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]==2.2.0) (3.4.2)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]==2.2.0) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]==2.2.0) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]==2.2.0) (0.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->ray[rllib]==2.2.0) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->ray[rllib]==2.2.0) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.4.3->ray[rllib]==2.2.0) (1.16.0)\n",
            "Downloading ray-2.2.0-cp310-cp310-manylinux2014_x86_64.whl (57.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.28.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.1-py3-none-any.whl size=701376 sha256=853bbd6a0930b336b64b55961fdd832a73e2ee113d9abb013b90f13b4dd68e71\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/00/fb/fe5cf2860fb9b7bc860e28f00095a1f42c7b726dd6f42d1acc\n",
            "Successfully built gym\n",
            "Installing collected packages: distlib, virtualenv, tensorboardX, lz4, gym, ray\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed distlib-0.3.9 gym-0.23.1 lz4-4.3.3 ray-2.2.0 tensorboardX-2.6.2.2 virtualenv-20.28.0\n"
          ]
        }
      ],
      "source": [
        "! pip install \"ray[rllib]==2.2.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b17e3aa",
      "metadata": {
        "id": "8b17e3aa"
      },
      "source": [
        "\n",
        "To import utility files for this chapter, on Colab you will also have to clone\n",
        "the repo and copy the code files to the base path of the runtime:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8206edba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8206edba",
        "outputId": "7c5ea9f6-543e-4857-982e-5f98699cbf91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'learning_ray'...\n",
            "remote: Enumerating objects: 1385, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 1385 (delta 42), reused 44 (delta 19), pack-reused 1313 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1385/1385), 119.42 MiB | 31.79 MiB/s, done.\n",
            "Resolving deltas: 100% (756/756), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/maxpumperla/learning_ray\n",
        "%cp -r learning_ray/notebooks/* ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4f8c77f4",
      "metadata": {
        "id": "4f8c77f4"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "\n",
        "class Env:\n",
        "\n",
        "    action_space: gym.spaces.Space\n",
        "    observation_space: gym.spaces.Space\n",
        "\n",
        "    def step(self, action):\n",
        "        ...\n",
        "\n",
        "    def reset(self):\n",
        "        ...\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f72525bf",
      "metadata": {
        "id": "f72525bf"
      },
      "source": [
        "In `maze.py` we set `num_rollout_workers=0` for this notebook, so that the code works in Colab. In the book itself we use 2 rollout workers to show that experience collection can be distributed by RLlib."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydantic==1.10.11\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTE7J0l91eqa",
        "outputId": "68354925-598c-4afb-e780-b040e079f770"
      },
      "id": "CTE7J0l91eqa",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic==1.10.11\n",
            "  Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (148 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/149.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m143.4/149.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.11) (4.12.2)\n",
            "Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/3.1 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydantic\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.9.2\n",
            "    Uninstalling pydantic-2.9.2:\n",
            "      Successfully uninstalled pydantic-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.20 requires pydantic>=2.7.0, but you have pydantic 1.10.11 which is incompatible.\n",
            "langchain 0.3.7 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.11 which is incompatible.\n",
            "langchain-core 0.3.19 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.11 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-1.10.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "83a6734a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83a6734a",
        "outputId": "48792384-d3e4-48f4-f282-b3509b38446c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "/usr/local/lib/python3.10/dist-packages/ray/air/_internal/remote_storage.py:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import packaging\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/tensorboardx.py:35: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  VALID_NP_HPARAMS = (np.bool8, np.float32, np.float64, np.int32, np.int64)\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "2024-11-30 16:11:13,270\tINFO worker.py:1538 -- Started a local Ray instance.\n",
            "2024-11-30 16:11:15,885\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
            "2024-11-30 16:11:15,892\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m   from jax import xla_computation as _xla_computation\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/air/_internal/remote_storage.py:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m   from pkg_resources import packaging\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m   declare_namespace(pkg)\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m   declare_namespace(pkg)\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m   declare_namespace(pkg)\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/logger/tensorboardx.py:35: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m   VALID_NP_HPARAMS = (np.bool8, np.float32, np.float64, np.int32, np.int64)\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=2224)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m 2024-11-30 16:11:43,318\tWARNING algorithm_config.py:488 -- Cannot create DQNConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m 2024-11-30 16:11:43,318\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m 2024-11-30 16:11:43,525\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m 2024-11-30 16:11:43,533\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/rllib/models/catalog.py:810: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   prep = cls(observation_space, options)\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/dqn/distributional_q_tf_model.py:70: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   super(DistributionalQTFModel, self).__init__(\n",
            "== Status ==\n",
            "Current time: 2024-11-30 16:11:43 (running for 00:00:28.07)\n",
            "Memory usage on this node: 2.6/12.7 GiB \n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/7.31 GiB heap, 0.0/3.65 GiB objects\n",
            "Result logdir: /root/ray_results/default\n",
            "Number of trials: 1/1 (1 RUNNING)\n",
            "\n",
            "\n",
            "2024-11-30 16:11:43,642\tERROR trial_runner.py:1088 -- Trial DQN_maze_gym_env.GymEnvironment_b9b22_00000: Error processing event.\n",
            "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/execution/ray_trial_executor.py\", line 1070, in get_next_executor_event\n",
            "    future_result = ray.get(ready_future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2311, in get\n",
            "    raise value\n",
            "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2224, ip=172.28.0.12, repr=DQN)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py\", line 441, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 169, in __init__\n",
            "    self.setup(copy.deepcopy(self.config))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py\", line 566, in setup\n",
            "    self.workers = WorkerSet(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\", line 169, in __init__\n",
            "    self._setup(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\", line 259, in _setup\n",
            "    self._local_worker = self._make_worker(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\", line 941, in _make_worker\n",
            "    worker = cls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 712, in __init__\n",
            "    self._build_policy_map(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1970, in _build_policy_map\n",
            "    self.policy_map.create_policy(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
            "    policy = create_policy_for_framework(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/policy.py\", line 117, in create_policy_for_framework\n",
            "    return policy_class(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/tf_policy_template.py\", line 258, in __init__\n",
            "    DynamicTFPolicy.__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 209, in __init__\n",
            "    self.model = make_model(self, obs_space, action_space, config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/dqn/dqn_tf_policy.py\", line 180, in build_q_model\n",
            "    q_model = ModelCatalog.get_model_v2(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/models/catalog.py\", line 709, in get_model_v2\n",
            "    return wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/dqn/distributional_q_tf_model.py\", line 165, in __init__\n",
            "    q_out = build_action_value(name + \"/action_value/\", self.model_out)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/dqn/distributional_q_tf_model.py\", line 135, in build_action_value\n",
            "    logits = tf.expand_dims(tf.ones_like(action_scores), -1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\", line 88, in wrapper\n",
            "    return op(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/common/keras_tensor.py\", line 138, in __tf_tensor__\n",
            "    raise ValueError(\n",
            "ValueError: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n",
            "\n",
            "```\n",
            "x = Input(...)\n",
            "...\n",
            "tf_fn(x)  # Invalid.\n",
            "```\n",
            "\n",
            "What you should do instead is wrap `tf_fn` in a layer:\n",
            "\n",
            "```\n",
            "class MyLayer(Layer):\n",
            "    def call(self, x):\n",
            "        return tf_fn(x)\n",
            "\n",
            "x = MyLayer()(x)\n",
            "```\n",
            "\n",
            "== Status ==\n",
            "Current time: 2024-11-30 16:11:43 (running for 00:00:28.08)\n",
            "Memory usage on this node: 2.6/12.7 GiB \n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/7.31 GiB heap, 0.0/3.65 GiB objects\n",
            "Result logdir: /root/ray_results/default\n",
            "Number of trials: 1/1 (1 ERROR)\n",
            "Number of errored trials: 1\n",
            "+---------------------------------------------+--------------+-------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                                  |   # failures | error file                                                                                            |\n",
            "|---------------------------------------------+--------------+-------------------------------------------------------------------------------------------------------|\n",
            "| DQN_maze_gym_env.GymEnvironment_b9b22_00000 |            1 | /root/ray_results/default/DQN_maze_gym_env.GymEnvironment_b9b22_00000_0_2024-11-30_16-11-16/error.txt |\n",
            "+---------------------------------------------+--------------+-------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "== Status ==\n",
            "Current time: 2024-11-30 16:11:43 (running for 00:00:28.08)\n",
            "Memory usage on this node: 2.6/12.7 GiB \n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/7.31 GiB heap, 0.0/3.65 GiB objects\n",
            "Result logdir: /root/ray_results/default\n",
            "Number of trials: 1/1 (1 ERROR)\n",
            "Number of errored trials: 1\n",
            "+---------------------------------------------+--------------+-------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                                  |   # failures | error file                                                                                            |\n",
            "|---------------------------------------------+--------------+-------------------------------------------------------------------------------------------------------|\n",
            "| DQN_maze_gym_env.GymEnvironment_b9b22_00000 |            1 | /root/ray_results/default/DQN_maze_gym_env.GymEnvironment_b9b22_00000_0_2024-11-30_16-11-16/error.txt |\n",
            "+---------------------------------------------+--------------+-------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "2024-11-30 16:11:43,657\tERROR ray_trial_executor.py:118 -- An exception occurred when trying to stop the Ray actor:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/execution/ray_trial_executor.py\", line 109, in _post_stop_cleanup\n",
            "    ray.get(future, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2311, in get\n",
            "    raise value\n",
            "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2224, ip=172.28.0.12, repr=DQN)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py\", line 441, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 169, in __init__\n",
            "    self.setup(copy.deepcopy(self.config))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py\", line 566, in setup\n",
            "    self.workers = WorkerSet(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\", line 169, in __init__\n",
            "    self._setup(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\", line 259, in _setup\n",
            "    self._local_worker = self._make_worker(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\", line 941, in _make_worker\n",
            "    worker = cls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 712, in __init__\n",
            "    self._build_policy_map(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1970, in _build_policy_map\n",
            "    self.policy_map.create_policy(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
            "    policy = create_policy_for_framework(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/policy.py\", line 117, in create_policy_for_framework\n",
            "    return policy_class(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/tf_policy_template.py\", line 258, in __init__\n",
            "    DynamicTFPolicy.__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 209, in __init__\n",
            "    self.model = make_model(self, obs_space, action_space, config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/dqn/dqn_tf_policy.py\", line 180, in build_q_model\n",
            "    q_model = ModelCatalog.get_model_v2(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/models/catalog.py\", line 709, in get_model_v2\n",
            "    return wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/dqn/distributional_q_tf_model.py\", line 165, in __init__\n",
            "    q_out = build_action_value(name + \"/action_value/\", self.model_out)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/dqn/distributional_q_tf_model.py\", line 135, in build_action_value\n",
            "    logits = tf.expand_dims(tf.ones_like(action_scores), -1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\", line 88, in wrapper\n",
            "    return op(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/common/keras_tensor.py\", line 138, in __tf_tensor__\n",
            "    raise ValueError(\n",
            "ValueError: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n",
            "\n",
            "```\n",
            "x = Input(...)\n",
            "...\n",
            "tf_fn(x)  # Invalid.\n",
            "```\n",
            "\n",
            "What you should do instead is wrap `tf_fn` in a layer:\n",
            "\n",
            "```\n",
            "class MyLayer(Layer):\n",
            "    def call(self, x):\n",
            "        return tf_fn(x)\n",
            "\n",
            "x = MyLayer()(x)\n",
            "```\n",
            "\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m 2024-11-30 16:11:43,631\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2224, ip=172.28.0.12, repr=DQN)\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py\", line 441, in __init__\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     super().__init__(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 169, in __init__\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py\", line 566, in setup\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     self.workers = WorkerSet(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\", line 169, in __init__\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     self._setup(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\", line 259, in _setup\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     self._local_worker = self._make_worker(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\", line 941, in _make_worker\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     worker = cls(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 712, in __init__\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     self._build_policy_map(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1970, in _build_policy_map\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     self.policy_map.create_policy(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     policy = create_policy_for_framework(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/policy.py\", line 117, in create_policy_for_framework\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     return policy_class(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/tf_policy_template.py\", line 258, in __init__\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     DynamicTFPolicy.__init__(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 209, in __init__\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     self.model = make_model(self, obs_space, action_space, config)\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/dqn/dqn_tf_policy.py\", line 180, in build_q_model\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     q_model = ModelCatalog.get_model_v2(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/models/catalog.py\", line 709, in get_model_v2\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     return wrapper(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/dqn/distributional_q_tf_model.py\", line 165, in __init__\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     q_out = build_action_value(name + \"/action_value/\", self.model_out)\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/dqn/distributional_q_tf_model.py\", line 135, in build_action_value\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     logits = tf.expand_dims(tf.ones_like(action_scores), -1)\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\", line 88, in wrapper\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     return op(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/common/keras_tensor.py\", line 138, in __tf_tensor__\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     raise ValueError(\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m ValueError: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m \n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m ```\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m x = Input(...)\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m ...\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m tf_fn(x)  # Invalid.\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m ```\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m \n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m What you should do instead is wrap `tf_fn` in a layer:\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m \n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m ```\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m class MyLayer(Layer):\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m     def call(self, x):\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m         return tf_fn(x)\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m \n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m x = MyLayer()(x)\n",
            "\u001b[2m\u001b[36m(DQN pid=2224)\u001b[0m ```\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/ray/rllib/\u001b[0m\u001b[1;33mtrain.py\u001b[0m:\u001b[94m191\u001b[0m in \u001b[92mfile\u001b[0m                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m temp_file:                                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   \u001b[0mtemp_file.close()                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m190 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m191 \u001b[2m│   \u001b[0mrun_rllib_experiments(                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m192 \u001b[0m\u001b[2m│   │   \u001b[0mexperiments=experiments,                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m193 \u001b[0m\u001b[2m│   │   \u001b[0mv=v,                                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m194 \u001b[0m\u001b[2m│   │   \u001b[0mvv=vv,                                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    algo = \u001b[33m'DQN'\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       checkpoint_at_end = \u001b[94mTrue\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       checkpoint_config = \u001b[1m{\u001b[0m                                                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'checkpoint_frequency'\u001b[0m: \u001b[94m0\u001b[0m,                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'checkpoint_at_end'\u001b[0m: \u001b[94mTrue\u001b[0m,                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'num_to_keep'\u001b[0m: \u001b[94mNone\u001b[0m,                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'checkpoint_score_attribute'\u001b[0m: \u001b[33m'training_iteration'\u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1m}\u001b[0m                                                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         checkpoint_freq = \u001b[94m0\u001b[0m                                                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   checkpoint_score_attr = \u001b[33m'training_iteration'\u001b[0m                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             config_file = \u001b[33m'maze.py'\u001b[0m                                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                exp_name = \u001b[33m'default'\u001b[0m                                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             experiments = \u001b[1m{\u001b[0m                                                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'default'\u001b[0m: \u001b[1m{\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'run'\u001b[0m: \u001b[33m'DQN'\u001b[0m,                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'config'\u001b[0m: \u001b[1m{\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'extra_python_environs_for_driver'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'extra_python_environs_for_worker'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'num_gpus'\u001b[0m: \u001b[94m0\u001b[0m,                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'num_cpus_per_worker'\u001b[0m: \u001b[94m1\u001b[0m,                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'num_gpus_per_worker'\u001b[0m: \u001b[94m0\u001b[0m,                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'_fake_gpus'\u001b[0m: \u001b[94mFalse\u001b[0m,                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'custom_resources_per_worker'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'placement_strategy'\u001b[0m: \u001b[33m'PACK'\u001b[0m,                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'eager_tracing'\u001b[0m: \u001b[94mFalse\u001b[0m,                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'eager_max_retraces'\u001b[0m: \u001b[94m20\u001b[0m,                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m...\u001b[0m +\u001b[94m113\u001b[0m                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[1m}\u001b[0m,                                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'stop'\u001b[0m: \u001b[1m{\u001b[0m\u001b[33m'timesteps_total'\u001b[0m: \u001b[94m10000\u001b[0m\u001b[1m}\u001b[0m,                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'checkpoint_config'\u001b[0m:                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1;35mCheckpointConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcheckpoint_score_attribute\u001b[0m=\u001b[33m'training_iteration'\u001b[0m,  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[33mcheckpoint_at_end\u001b[0m=\u001b[94mTrue\u001b[0m\u001b[1m)\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[1m}\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1m}\u001b[0m                                                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               file_type = \u001b[1m<\u001b[0m\u001b[1;95mSupportedFileType.python:\u001b[0m\u001b[39m \u001b[0m\u001b[33m'python'\u001b[0m\u001b[1m>\u001b[0m                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               framework = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    keep_checkpoints_num = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              local_mode = \u001b[94mFalse\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             ray_address = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            ray_num_cpus = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            ray_num_gpus = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           ray_num_nodes = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m ray_object_store_memory = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  ray_ui = \u001b[94mFalse\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  resume = \u001b[94mFalse\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               scheduler = \u001b[33m'FIFO'\u001b[0m                                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        scheduler_config = \u001b[33m'\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[33m'\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    stop = \u001b[33m'\u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m\"timesteps_total\": 10000\u001b[0m\u001b[1;33m}\u001b[0m\u001b[33m'\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               temp_file = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   trace = \u001b[94mFalse\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       v = \u001b[94mFalse\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      vv = \u001b[94mFalse\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/ray/rllib/\u001b[0m\u001b[1;33mtrain.py\u001b[0m:\u001b[94m388\u001b[0m in \u001b[92mrun_rllib_experiments\u001b[0m          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m385 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m386 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Run the Tune experiment and return the trials.\u001b[0m                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m387 \u001b[0m\u001b[2m│   \u001b[0mscheduler_config = json.loads(scheduler_config)                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m388 \u001b[2m│   \u001b[0mtrials = run_experiments(                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m389 \u001b[0m\u001b[2m│   │   \u001b[0mexperiments,                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m390 \u001b[0m\u001b[2m│   │   \u001b[0mscheduler=create_scheduler(scheduler, **scheduler_config),                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m391 \u001b[0m\u001b[2m│   │   \u001b[0mresume=resume,                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    algo = \u001b[33m'DQN'\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     exp = \u001b[1m{\u001b[0m                                                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'run'\u001b[0m: \u001b[33m'DQN'\u001b[0m,                                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'config'\u001b[0m: \u001b[1m{\u001b[0m                                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'extra_python_environs_for_driver'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'extra_python_environs_for_worker'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'num_gpus'\u001b[0m: \u001b[94m0\u001b[0m,                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'num_cpus_per_worker'\u001b[0m: \u001b[94m1\u001b[0m,                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'num_gpus_per_worker'\u001b[0m: \u001b[94m0\u001b[0m,                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'_fake_gpus'\u001b[0m: \u001b[94mFalse\u001b[0m,                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'custom_resources_per_worker'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'placement_strategy'\u001b[0m: \u001b[33m'PACK'\u001b[0m,                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'eager_tracing'\u001b[0m: \u001b[94mFalse\u001b[0m,                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'eager_max_retraces'\u001b[0m: \u001b[94m20\u001b[0m,                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m...\u001b[0m +\u001b[94m113\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[1m}\u001b[0m,                                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'stop'\u001b[0m: \u001b[1m{\u001b[0m\u001b[33m'timesteps_total'\u001b[0m: \u001b[94m10000\u001b[0m\u001b[1m}\u001b[0m,                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'checkpoint_config'\u001b[0m:                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1;35mCheckpointConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcheckpoint_score_attribute\u001b[0m=\u001b[33m'training_iteration'\u001b[0m,  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[33mcheckpoint_at_end\u001b[0m=\u001b[94mTrue\u001b[0m\u001b[1m)\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1m}\u001b[0m                                                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             experiments = \u001b[1m{\u001b[0m                                                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'default'\u001b[0m: \u001b[1m{\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'run'\u001b[0m: \u001b[33m'DQN'\u001b[0m,                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'config'\u001b[0m: \u001b[1m{\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'extra_python_environs_for_driver'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'extra_python_environs_for_worker'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'num_gpus'\u001b[0m: \u001b[94m0\u001b[0m,                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'num_cpus_per_worker'\u001b[0m: \u001b[94m1\u001b[0m,                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'num_gpus_per_worker'\u001b[0m: \u001b[94m0\u001b[0m,                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'_fake_gpus'\u001b[0m: \u001b[94mFalse\u001b[0m,                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'custom_resources_per_worker'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'placement_strategy'\u001b[0m: \u001b[33m'PACK'\u001b[0m,                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'eager_tracing'\u001b[0m: \u001b[94mFalse\u001b[0m,                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'eager_max_retraces'\u001b[0m: \u001b[94m20\u001b[0m,                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m...\u001b[0m +\u001b[94m113\u001b[0m                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[1m}\u001b[0m,                                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'stop'\u001b[0m: \u001b[1m{\u001b[0m\u001b[33m'timesteps_total'\u001b[0m: \u001b[94m10000\u001b[0m\u001b[1m}\u001b[0m,                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'checkpoint_config'\u001b[0m:                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1;35mCheckpointConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcheckpoint_score_attribute\u001b[0m=\u001b[33m'training_iteration'\u001b[0m,  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[33mcheckpoint_at_end\u001b[0m=\u001b[94mTrue\u001b[0m\u001b[1m)\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[1m}\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1m}\u001b[0m                                                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               framework = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  input_ = \u001b[33m'sampler'\u001b[0m                                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              local_mode = \u001b[94mFalse\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             ray_address = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            ray_num_cpus = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            ray_num_gpus = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           ray_num_nodes = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m ray_object_store_memory = \u001b[94mNone\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  ray_ui = \u001b[94mFalse\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  resume = \u001b[94mFalse\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               scheduler = \u001b[33m'FIFO'\u001b[0m                                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        scheduler_config = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   trace = \u001b[94mFalse\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       v = \u001b[94mFalse\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 verbose = \u001b[94m1\u001b[0m                                                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      vv = \u001b[94mFalse\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/ray/tune/\u001b[0m\u001b[1;33mtune.py\u001b[0m:\u001b[94m852\u001b[0m in \u001b[92mrun_experiments\u001b[0m                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m849 \u001b[0m\u001b[2m│   \u001b[0mexperiments = _convert_to_experiment_list(experiments)                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m850 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m851 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m concurrent:                                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m852 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m run(                                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m853 \u001b[0m\u001b[2m│   │   │   \u001b[0mexperiments,                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m854 \u001b[0m\u001b[2m│   │   │   \u001b[0mserver_port=server_port,                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m855 \u001b[0m\u001b[2m│   │   │   \u001b[0mverbose=verbose,                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               _remote = \u001b[94mFalse\u001b[0m                                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             callbacks = \u001b[94mNone\u001b[0m                                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            concurrent = \u001b[94mTrue\u001b[0m                                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           experiments = \u001b[1m[\u001b[0m                                                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         \u001b[2m│   \u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mray.tune.experiment.experiment.Experiment\u001b[0m\u001b[39m object at \u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         \u001b[94m0x7c1fc3ab53c0\u001b[0m\u001b[1m>\u001b[0m                                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         \u001b[1m]\u001b[0m                                                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     progress_reporter = \u001b[94mNone\u001b[0m                                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m raise_on_failed_trial = \u001b[94mTrue\u001b[0m                                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                resume = \u001b[94mFalse\u001b[0m                                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          reuse_actors = \u001b[94mNone\u001b[0m                                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             scheduler = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.schedulers.trial_scheduler.FIFOScheduler\u001b[0m\u001b[39m object at \u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         \u001b[94m0x7c1fc3ab7a00\u001b[0m\u001b[1m>\u001b[0m                                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           server_port = \u001b[94mNone\u001b[0m                                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        trial_executor = \u001b[94mNone\u001b[0m                                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               verbose = \u001b[94m1\u001b[0m                                                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/ray/tune/\u001b[0m\u001b[1;33mtune.py\u001b[0m:\u001b[94m756\u001b[0m in \u001b[92mrun\u001b[0m                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m753 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m754 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m incomplete_trials:                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m755 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m raise_on_failed_trial \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m state[\u001b[33m\"\u001b[0m\u001b[33msignal\u001b[0m\u001b[33m\"\u001b[0m]:                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m756 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m TuneError(\u001b[33m\"\u001b[0m\u001b[33mTrials did not complete\u001b[0m\u001b[33m\"\u001b[0m, incomplete_trials)                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m757 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m758 \u001b[0m\u001b[2m│   │   │   \u001b[0mlogger.error(\u001b[33m\"\u001b[0m\u001b[33mTrials did not complete: \u001b[0m\u001b[33m%s\u001b[0m\u001b[33m\"\u001b[0m, incomplete_trials)                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m759 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m _experiment_checkpoint_dir = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    _remote = \u001b[94mFalse\u001b[0m                                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       _remote_string_queue = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  all_start = \u001b[94m1732983075.5337849\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      allow_signal_catching = \u001b[94mTrue\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   callback = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.syncer.SyncerCallback\u001b[0m\u001b[39m object at \u001b[0m\u001b[94m0x7c1fc3b044f0\u001b[0m\u001b[1m>\u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  callbacks = \u001b[1m[\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2m│   \u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mray.tune.progress_reporter.TrialProgressCallback\u001b[0m\u001b[39m object at\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[94m0x7c1fc3b04340\u001b[0m\u001b[39m>,\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2;39m│   \u001b[0m\u001b[39m<ray.tune.logger.csv.CSVLoggerCallback object at \u001b[0m           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[94m0x7c1fc3b043d0\u001b[0m\u001b[39m>,\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2;39m│   \u001b[0m\u001b[39m<ray.tune.logger.json.JsonLoggerCallback object at \u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[94m0x7c1fc3b04430\u001b[0m\u001b[39m>,\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2;39m│   \u001b[0m\u001b[39m<ray.tune.logger.tensorboardx.TBXLoggerCallback object at \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[94m0x7c1fc3b04490\u001b[0m\u001b[39m>,\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2;39m│   \u001b[0m\u001b[39m<ray.tune.syncer.SyncerCallback object at \u001b[0m\u001b[94m0x7c1fc3b044f0\u001b[0m\u001b[1m>\u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[1m]\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         chdir_to_trial_dir = \u001b[94mTrue\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          checkpoint_at_end = \u001b[94mFalse\u001b[0m                                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          checkpoint_config = \u001b[1;35mCheckpointConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcheckpoint_score_attribute\u001b[0m=\u001b[33m''\u001b[0m,                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[33mcheckpoint_at_end\u001b[0m=\u001b[94mFalse\u001b[0m\u001b[1m)\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            checkpoint_freq = \u001b[94m0\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      checkpoint_score_attr = \u001b[33m''\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     checkpoint_score_order = \u001b[33m'max'\u001b[0m                                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     config = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        exp = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.experiment.experiment.Experiment\u001b[0m\u001b[39m object at \u001b[0m           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[94m0x7c1fc3ab53c0\u001b[0m\u001b[1m>\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                experiments = \u001b[1m[\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2m│   \u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mray.tune.experiment.experiment.Experiment\u001b[0m\u001b[39m object at \u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[94m0x7c1fc3ab53c0\u001b[0m\u001b[1m>\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[1m]\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             export_formats = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  fail_fast = \u001b[94mFalse\u001b[0m                                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          i = \u001b[94m0\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          incomplete_trials = \u001b[1m[\u001b[0mDQN_maze_gym_env.GymEnvironment_b9b22_00000\u001b[1m]\u001b[0m                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              is_local_mode = \u001b[94mFalse\u001b[0m                                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       keep_checkpoints_num = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  local_dir = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                log_to_file = \u001b[94mFalse\u001b[0m                                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      max_concurrent_trials = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               max_failures = \u001b[94m0\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     metric = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       mode = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       name = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                num_samples = \u001b[94m1\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           original_handler = \u001b[1m<\u001b[0m\u001b[1;95mbuilt-in\u001b[0m\u001b[39m function default_int_handler\u001b[0m\u001b[1m>\u001b[0m                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           progress_metrics = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          progress_reporter = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.progress_reporter.CLIReporter\u001b[0m\u001b[39m object at \u001b[0m              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[94m0x7c1fc3b42680\u001b[0m\u001b[1m>\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      raise_on_failed_trial = \u001b[94mTrue\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        resources_per_trial = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    restore = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       result_buffer_length = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     resume = \u001b[94mFalse\u001b[0m                                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               reuse_actors = \u001b[94mFalse\u001b[0m                                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          run_or_experiment = \u001b[1m[\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2m│   \u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mray.tune.experiment.experiment.Experiment\u001b[0m\u001b[39m object at \u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[94m0x7c1fc3ab53c0\u001b[0m\u001b[1m>\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[1m]\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     runner = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.execution.trial_runner.TrialRunner\u001b[0m\u001b[39m object at \u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[94m0x7c1fc3b045b0\u001b[0m\u001b[1m>\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  scheduler = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.schedulers.trial_scheduler.FIFOScheduler\u001b[0m\u001b[39m object at \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[94m0x7c1fc3ab7a00\u001b[0m\u001b[1m>\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 search_alg = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.search.basic_variant.BasicVariantGenerator\u001b[0m\u001b[39m object at \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[94m0x7c1fc3b041f0\u001b[0m\u001b[1m>\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                server_port = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      state = \u001b[1m{\u001b[0m\u001b[33m'signal'\u001b[0m: \u001b[94mNone\u001b[0m\u001b[1m}\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       stop = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                sync_config = \u001b[1;35mSyncConfig\u001b[0m\u001b[1m(\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2m│   \u001b[0m\u001b[33mupload_dir\u001b[0m=\u001b[94mNone\u001b[0m,                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2m│   \u001b[0m\u001b[33msyncer\u001b[0m=\u001b[33m'auto'\u001b[0m,                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2m│   \u001b[0m\u001b[33msync_on_checkpoint\u001b[0m=\u001b[94mTrue\u001b[0m,                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2m│   \u001b[0m\u001b[33msync_period\u001b[0m=\u001b[94m300\u001b[0m,                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2m│   \u001b[0m\u001b[33msync_timeout\u001b[0m=\u001b[94m1800\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[1m)\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              time_budget_s = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  trainable = \u001b[1m[\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[2m│   \u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mray.tune.experiment.experiment.Experiment\u001b[0m\u001b[39m object at \u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[94m0x7c1fc3ab53c0\u001b[0m\u001b[1m>\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[1m]\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      trial = DQN_maze_gym_env.GymEnvironment_b9b22_00000                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      trial_dirname_creator = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trial_executor = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.execution.ray_trial_executor.RayTrialExecutor\u001b[0m\u001b[39m object \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              \u001b[39mat \u001b[0m\u001b[94m0x7c1fc3b04190\u001b[0m\u001b[1m>\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         trial_name_creator = \u001b[94mNone\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 tune_start = \u001b[94m1732983075.57236\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 tune_taken = \u001b[94m28.08103847503662\u001b[0m                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    verbose = \u001b[94m1\u001b[0m                                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mTuneError: \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'Trials did not complete'\u001b[0m, \u001b[1m[\u001b[0mDQN_maze_gym_env.GymEnvironment_b9b22_00000\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! rllib train file maze.py --stop '{\"timesteps_total\": 10000}'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8dfc658",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "e8dfc658"
      },
      "source": [
        "\n",
        "Try:\n",
        "rllib evaluate ~/ray_results/maze_env/<checkpoint>\\\n",
        " --algo DQN\\\n",
        " --env maze_gym_env.Environment\\\n",
        " --steps 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965ee003",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "965ee003"
      },
      "outputs": [],
      "source": [
        "from ray.tune.logger import pretty_print\n",
        "from maze_gym_env import GymEnvironment\n",
        "from ray.rllib.algorithms.dqn import DQNConfig\n",
        "\n",
        "config = (DQNConfig().environment(GymEnvironment)\n",
        "          .rollouts(num_rollout_workers=2, create_env_on_local_worker=True))\n",
        "\n",
        "pretty_print(config.to_dict())\n",
        "\n",
        "algo = config.build()\n",
        "\n",
        "for i in range(10):\n",
        "    result = algo.train()\n",
        "\n",
        "print(pretty_print(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "283bf0f8",
      "metadata": {
        "id": "283bf0f8"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.algorithm import Algorithm\n",
        "\n",
        "\n",
        "checkpoint = algo.save()\n",
        "print(checkpoint)\n",
        "\n",
        "evaluation = algo.evaluate()\n",
        "print(pretty_print(evaluation))\n",
        "\n",
        "algo.stop()\n",
        "restored_algo = Algorithm.from_checkpoint(checkpoint)\n",
        "\n",
        "algo = restored_algo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e3c637",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "e4e3c637"
      },
      "outputs": [],
      "source": [
        "env = GymEnvironment()\n",
        "done = False\n",
        "total_reward = 0\n",
        "observations = env.reset()\n",
        "\n",
        "while not done:\n",
        "    action = algo.compute_single_action(observations)\n",
        "    observations, reward, done, info = env.step(action)\n",
        "    total_reward += reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6230b49d",
      "metadata": {
        "id": "6230b49d"
      },
      "outputs": [],
      "source": [
        "action = algo.compute_actions(\n",
        "    {\"obs_1\": observations, \"obs_2\": observations}\n",
        ")\n",
        "print(action)\n",
        "# {'obs_1': 0, 'obs_2': 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d2c31a",
      "metadata": {
        "id": "51d2c31a"
      },
      "outputs": [],
      "source": [
        "policy = algo.get_policy()\n",
        "print(policy.get_weights())\n",
        "\n",
        "model = policy.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83366455",
      "metadata": {
        "id": "83366455"
      },
      "outputs": [],
      "source": [
        "workers = algo.workers\n",
        "workers.foreach_worker(\n",
        "    lambda remote_trainer: remote_trainer.get_policy().get_weights()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "371e167b",
      "metadata": {
        "id": "371e167b"
      },
      "outputs": [],
      "source": [
        "model.base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a539340",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "5a539340"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.models.preprocessors import get_preprocessor\n",
        "\n",
        "\n",
        "env = GymEnvironment()\n",
        "obs_space = env.observation_space\n",
        "preprocessor = get_preprocessor(obs_space)(obs_space)\n",
        "\n",
        "observations = env.reset()\n",
        "transformed = preprocessor.transform(observations).reshape(1, -1)\n",
        "\n",
        "model_output, _ = model({\"obs\": transformed})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f47a55f",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "0f47a55f"
      },
      "outputs": [],
      "source": [
        "q_values = model.get_q_value_distributions(model_output)\n",
        "print(q_values)\n",
        "\n",
        "action_distribution = policy.dist_class(model_output, model)\n",
        "sample = action_distribution.sample()\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94dc4ca5",
      "metadata": {
        "id": "94dc4ca5"
      },
      "source": [
        "\n",
        "![RLlib Environments](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_04/rllib_envs.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9081e6a5",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "9081e6a5"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "from gym.spaces import Discrete\n",
        "import os\n",
        "\n",
        "\n",
        "class MultiAgentMaze(MultiAgentEnv):\n",
        "\n",
        "    def __init__(self,  *args, **kwargs):\n",
        "        self.action_space = Discrete(4)\n",
        "        self.observation_space = Discrete(5*5)\n",
        "        self.agents = {1: (4, 0), 2: (0, 4)}\n",
        "        self.goal = (4, 4)\n",
        "        self.info = {1: {'obs': self.agents[1]}, 2: {'obs': self.agents[2]}}\n",
        "\n",
        "    def reset(self):\n",
        "        self.agents = {1: (4, 0), 2: (0, 4)}\n",
        "\n",
        "        return {1: self.get_observation(1), 2: self.get_observation(2)}\n",
        "\n",
        "    def get_observation(self, agent_id):\n",
        "        seeker = self.agents[agent_id]\n",
        "        return 5 * seeker[0] + seeker[1]\n",
        "\n",
        "    def get_reward(self, agent_id):\n",
        "        return 1 if self.agents[agent_id] == self.goal else 0\n",
        "\n",
        "    def is_done(self, agent_id):\n",
        "        return self.agents[agent_id] == self.goal\n",
        "\n",
        "    def step(self, action):\n",
        "        agent_ids = action.keys()\n",
        "\n",
        "        for agent_id in agent_ids:\n",
        "            seeker = self.agents[agent_id]\n",
        "            if action[agent_id] == 0:  # move down\n",
        "                seeker = (min(seeker[0] + 1, 4), seeker[1])\n",
        "            elif action[agent_id] == 1:  # move left\n",
        "                seeker = (seeker[0], max(seeker[1] - 1, 0))\n",
        "            elif action[agent_id] == 2:  # move up\n",
        "                seeker = (max(seeker[0] - 1, 0), seeker[1])\n",
        "            elif action[agent_id] == 3:  # move right\n",
        "                seeker = (seeker[0], min(seeker[1] + 1, 4))\n",
        "            else:\n",
        "                raise ValueError(\"Invalid action\")\n",
        "            self.agents[agent_id] = seeker\n",
        "\n",
        "        observations = {i: self.get_observation(i) for i in agent_ids}\n",
        "        rewards = {i: self.get_reward(i) for i in agent_ids}\n",
        "        done = {i: self.is_done(i) for i in agent_ids}\n",
        "\n",
        "        done[\"__all__\"] = all(done.values())\n",
        "\n",
        "        return observations, rewards, done, self.info\n",
        "\n",
        "    def render(self, *args, **kwargs):\n",
        "        \"\"\"We override this method here so clear the output in Jupyter notebooks.\n",
        "        The previous implementation works well in the terminal, but does not clear\n",
        "        the screen in interactive environments.\n",
        "        \"\"\"\n",
        "        os.system('cls' if os.name == 'nt' else 'clear')\n",
        "        try:\n",
        "            from IPython.display import clear_output\n",
        "            clear_output(wait=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
        "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
        "        grid[self.agents[1][0]][self.agents[1][1]] = '|1'\n",
        "        grid[self.agents[2][0]][self.agents[2][1]] = '|2'\n",
        "        grid[self.agents[2][0]][self.agents[2][1]] = '|2'\n",
        "        print(''.join([''.join(grid_row) for grid_row in grid]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3057fa",
      "metadata": {
        "id": "7a3057fa"
      },
      "source": [
        "![RLlib Mapping Envs](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_04/mapping_envs.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b74fd5b",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "3b74fd5b"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "env = MultiAgentMaze()\n",
        "\n",
        "while True:\n",
        "    obs, rew, done, info = env.step(\n",
        "        {1: env.action_space.sample(), 2: env.action_space.sample()}\n",
        "    )\n",
        "    time.sleep(0.1)\n",
        "    env.render()\n",
        "    if any(done.values()):\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417642b6",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "417642b6"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.dqn import DQNConfig\n",
        "\n",
        "simple_trainer = DQNConfig().environment(env=MultiAgentMaze).build()\n",
        "simple_trainer.train()\n",
        "\n",
        "algo = DQNConfig()\\\n",
        "    .environment(env=MultiAgentMaze)\\\n",
        "    .multi_agent(\n",
        "        policies={\n",
        "            \"policy_1\": (\n",
        "                None, env.observation_space, env.action_space, {\"gamma\": 0.80}\n",
        "            ),\n",
        "            \"policy_2\": (\n",
        "                None, env.observation_space, env.action_space, {\"gamma\": 0.95}\n",
        "            ),\n",
        "        },\n",
        "        policy_mapping_fn = lambda agent_id: f\"policy_{agent_id}\",\n",
        "    ).build()\n",
        "\n",
        "print(algo.train())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b65341b",
      "metadata": {
        "id": "9b65341b"
      },
      "source": [
        "![RLlib External Envs](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_04/rllib_external.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "897b4d78",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "897b4d78"
      },
      "outputs": [],
      "source": [
        "from gym.spaces import Discrete\n",
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "class AdvancedEnv(GymEnvironment):\n",
        "\n",
        "    def __init__(self, seeker=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.maze_len = 11\n",
        "        self.action_space = Discrete(4)\n",
        "        self.observation_space = Discrete(self.maze_len * self.maze_len)\n",
        "\n",
        "        if seeker:\n",
        "            assert 0 <= seeker[0] < self.maze_len and \\\n",
        "                   0 <= seeker[1] < self.maze_len\n",
        "            self.seeker = seeker\n",
        "        else:\n",
        "            self.reset()\n",
        "\n",
        "        self.goal = (self.maze_len-1, self.maze_len-1)\n",
        "        self.info = {'seeker': self.seeker, 'goal': self.goal}\n",
        "\n",
        "        self.punish_states = [\n",
        "            (i, j) for i in range(self.maze_len) for j in range(self.maze_len)\n",
        "            if i % 2 == 1 and j % 2 == 0\n",
        "        ]\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset seeker position randomly, return observations.\"\"\"\n",
        "        self.seeker = (\n",
        "            random.randint(0, self.maze_len - 1),\n",
        "            random.randint(0, self.maze_len - 1)\n",
        "        )\n",
        "        return self.get_observation()\n",
        "\n",
        "    def get_observation(self):\n",
        "        \"\"\"Encode the seeker position as integer\"\"\"\n",
        "        return self.maze_len * self.seeker[0] + self.seeker[1]\n",
        "\n",
        "    def get_reward(self):\n",
        "        \"\"\"Reward finding the goal and punish forbidden states\"\"\"\n",
        "        reward = -1 if self.seeker in self.punish_states else 0\n",
        "        reward += 5 if self.seeker == self.goal else 0\n",
        "        return reward\n",
        "\n",
        "    def render(self, *args, **kwargs):\n",
        "        \"\"\"We override this method here so clear the output in Jupyter notebooks.\n",
        "        The previous implementation works well in the terminal, but does not clear\n",
        "        the screen in interactive environments.\n",
        "        \"\"\"\n",
        "        os.system('cls' if os.name == 'nt' else 'clear')\n",
        "        try:\n",
        "            from IPython.display import clear_output\n",
        "            clear_output(wait=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "        grid = [['| ' for _ in range(self.maze_len)] +\n",
        "                [\"|\\n\"] for _ in range(self.maze_len)]\n",
        "        for punish in self.punish_states:\n",
        "            grid[punish[0]][punish[1]] = '|X'\n",
        "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
        "        grid[self.seeker[0]][self.seeker[1]] = '|S'\n",
        "        print(''.join([''.join(grid_row) for grid_row in grid]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeec1d65",
      "metadata": {
        "id": "eeec1d65"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.env.apis.task_settable_env import TaskSettableEnv\n",
        "\n",
        "\n",
        "class CurriculumEnv(AdvancedEnv, TaskSettableEnv):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        AdvancedEnv.__init__(self)\n",
        "\n",
        "    def difficulty(self):\n",
        "        return abs(self.seeker[0] - self.goal[0]) + \\\n",
        "               abs(self.seeker[1] - self.goal[1])\n",
        "\n",
        "    def get_task(self):\n",
        "        return self.difficulty()\n",
        "\n",
        "    def set_task(self, task_difficulty):\n",
        "        while not self.difficulty() <= task_difficulty:\n",
        "            self.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36dcd250",
      "metadata": {
        "id": "36dcd250"
      },
      "outputs": [],
      "source": [
        "def curriculum_fn(train_results, task_settable_env, env_ctx):\n",
        "    time_steps = train_results.get(\"timesteps_total\")\n",
        "    difficulty = time_steps // 1000\n",
        "    print(f\"Current difficulty: {difficulty}\")\n",
        "    return difficulty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db20214",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "4db20214"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.dqn import DQNConfig\n",
        "import tempfile\n",
        "\n",
        "\n",
        "temp = tempfile.mkdtemp()\n",
        "\n",
        "trainer = (\n",
        "    DQNConfig()\n",
        "    .environment(env=CurriculumEnv, env_task_fn=curriculum_fn)\n",
        "    .offline_data(output=temp)\n",
        "    .build()\n",
        ")\n",
        "\n",
        "for i in range(15):\n",
        "    trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e878b6b2",
      "metadata": {
        "id": "e878b6b2"
      },
      "outputs": [],
      "source": [
        "imitation_algo = (\n",
        "    DQNConfig()\n",
        "    .environment(env=AdvancedEnv)\n",
        "    .evaluation(off_policy_estimation_methods={})\n",
        "    .offline_data(input_=temp)\n",
        "    .exploration(explore=False)\n",
        "    .build())\n",
        "\n",
        "for i in range(10):\n",
        "    imitation_algo.train()\n",
        "\n",
        "imitation_algo.evaluate()"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}